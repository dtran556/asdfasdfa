{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMyMdWKwoqFZpRqGaCBW7S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtran556/asdfasdfa/blob/main/text_to_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Foward Diffusion\n",
        "Grab an image and then iteratively add more noise to the image until the image is fully noisy.\n",
        "\n",
        "Backwards Diffusion\n",
        "Grab an image and then iteratively reduce noise until the noisy image is pure."
      ],
      "metadata": {
        "id": "IBD88xdPPU1Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "oVlfF8evPQyz",
        "outputId": "e92af2c6-cf1e-416a-d5b5-8871e0889518"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4290598771.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDDPMScheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import DDPMScheduler\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "maxSteps = 1000\n",
        "noiseScheduler = DDPMScheduler(num_train_timesteps=maxSteps,\n",
        "                               beta_start=0.001,\n",
        "                               beta_end=0.02)\n",
        "\n",
        "def load_img():\n",
        "    image_path = \"/content/Sloth-RF.jpg\"\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    return transform(image).numpy()\n",
        "\n",
        "def plot_images(timesteps, images, title=\"Images\"):\n",
        "    fig, axes = plt.subplots(1, len(timesteps), figsize=(15, 3))\n",
        "    for i, (t, img) in enumerate(zip(timesteps, images)):\n",
        "        img_display = np.transpose(img, (1, 2, 0))\n",
        "        img_display = np.clip(img_display, 0, 1)\n",
        "        axes[i].imshow(img_display)\n",
        "        axes[i].set_title(f\"t={t}\")\n",
        "        axes[i].axis('off')\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "image = torch.tensor(load_img())\n",
        "forward_timesteps = torch.arange(0, maxSteps, maxSteps // 10)\n",
        "print(f\"Forward timesteps: {forward_timesteps}\")\n",
        "\n",
        "noise = torch.randn(image.shape)\n",
        "noisyImage = noiseScheduler.add_noise(\n",
        "    image.unsqueeze(0),\n",
        "    noise.unsqueeze(0),\n",
        "    forward_timesteps.unsqueeze(0)\n",
        ").squeeze(0).numpy()\n",
        "\n",
        "plot_images(forward_timesteps, noisyImage, \"Forward Diffusion\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import DDPMScheduler\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n"
      ],
      "metadata": {
        "id": "vWW3k5K4PXlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "celeb_dataset = load_dataset(\"flwrlabs/celeba\", split=\"train\")\n",
        "food_dataset = load_dataset(\"food101\", split=\"train\")\n",
        "cifar_dataset = load_dataset(\"cifar10\", split=\"train\")\n",
        "flowers_dataset = load_dataset(\"Voxel51/OxfordFlowers102\", split=\"train\")\n",
        "\n",
        "print(f\"CelebA: {len(celeb_dataset)} images\")\n",
        "print(f\"Food-101: {len(food_dataset)} images\")\n",
        "print(f\"CIFAR-10: {len(cifar_dataset)} images\")\n",
        "print(f\"Flowers-102: {len(flowers_dataset)} images\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "class CombinedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, celeb_data, food_data, cifar_data, flowers_data, transform=None):\n",
        "        self.celeb_data = celeb_data\n",
        "        self.food_data = food_data\n",
        "        self.cifar_data = cifar_data\n",
        "        self.flowers_data = flowers_data\n",
        "        self.transform = transform\n",
        "\n",
        "        self.celeb_len = len(celeb_data)\n",
        "        self.food_len = len(food_data)\n",
        "        self.cifar_len = len(cifar_data)\n",
        "        self.flowers_len = len(flowers_data)\n",
        "        self.total_len = self.celeb_len + self.food_len + self.cifar_len + self.flowers_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < self.celeb_len:\n",
        "            item = self.celeb_data[idx]\n",
        "            image = item['image']\n",
        "        elif idx < self.celeb_len + self.food_len:\n",
        "            item = self.food_data[idx - self.celeb_len]\n",
        "            image = item['image']\n",
        "        elif idx < self.celeb_len + self.food_len + self.cifar_len:\n",
        "            item = self.cifar_data[idx - self.celeb_len - self.food_len]\n",
        "            image = item['img']\n",
        "        else:\n",
        "            item = self.flowers_data[idx - self.celeb_len - self.food_len - self.cifar_len]\n",
        "            image = item['image']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "combined_dataset = CombinedDataset(celeb_dataset, food_dataset, cifar_dataset, flowers_dataset, transform=transform)\n",
        "dataloader = DataLoader(combined_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "print(f\"Combined dataset: {len(combined_dataset)} total images\")\n",
        "\n",
        "print(\"Sample images from combined dataset:\")\n",
        "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
        "for i, image in enumerate(dataloader):\n",
        "    if i >= 5:\n",
        "        break\n",
        "\n",
        "    img_display = image.squeeze(0).numpy()\n",
        "    img_display = np.transpose(img_display, (1, 2, 0))\n",
        "    img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "    axes[i].imshow(img_display)\n",
        "    axes[i].set_title(f\"Image {i+1}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1OLDvyI7Pd8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "\n",
        "        self.time_embed = nn.Linear(1, 256)\n",
        "\n",
        "        self.upconv1 = nn.Conv2d(256, 128, 3, padding=1)\n",
        "\n",
        "        self.upconv2 = nn.Conv2d(128, 64, 3, padding=1)\n",
        "        self.out = nn.Conv2d(64, 3, 3, padding=1)\n",
        "\n",
        "    def forward(self, x, timestep):\n",
        "        # Downsampling\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "\n",
        "        # Time embedding\n",
        "        t_emb = self.time_embed(timestep.float().unsqueeze(-1))\n",
        "        t_emb = t_emb.view(-1, 256, 1, 1)\n",
        "        x = x + t_emb\n",
        "\n",
        "        # Upsampling\n",
        "        x = torch.relu(self.upconv1(x))\n",
        "        x = torch.relu(self.upconv2(x))\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "wk45zcAUPgC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxSteps = 1000\n",
        "noiseScheduler = DDPMScheduler(num_train_timesteps=maxSteps,\n",
        "                               beta_start=0.001,\n",
        "                               beta_end=0.02)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "\n",
        "\n",
        "unet_model = SimpleUNet().to(device)\n",
        "optimizer = Adam(unet_model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "max_timesteps = 1000\n",
        "num_epochs = 25\n",
        "batch_size = 128\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in unet_model.parameters()):,}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch_idx, images in enumerate(dataloader):\n",
        "        # Load Data\n",
        "        images = images.to(device)\n",
        "        img_shape = images.shape  # [Batch Size, Num Channels, W, H]\n",
        "\n",
        "        # Run augmentations (simple random horizontal flip)\n",
        "        if torch.rand(1) > 0.5:\n",
        "            images = torch.flip(images, dims=[3])\n",
        "\n",
        "        # Forward Diffusion\n",
        "        # Generate Random Noise for Random timesteps\n",
        "        timesteps = torch.randint(0, max_timesteps, (len(images),)).to(device)\n",
        "        noise = torch.randn(img_shape).to(device)\n",
        "\n",
        "        # Add noise to images\n",
        "        noisy_images = noiseScheduler.add_noise(images, noise, timesteps)\n",
        "\n",
        "        # UNet Forward Pass\n",
        "        prediction = unet_model(noisy_images, timesteps)\n",
        "\n",
        "        # Calculate Loss\n",
        "        loss = F.mse_loss(prediction, noise)\n",
        "\n",
        "        # Update Weights\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_loss = epoch_loss / num_batches\n",
        "    print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "torch.save(unet_model.state_dict(), 'genImage25epoch.pth')\n",
        "torch.save({\n",
        "    'model_state_dict': unet_model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'epoch': num_epochs,\n",
        "    'loss': avg_loss\n",
        "},'trained_unet_path.pth')\n",
        "\n",
        "print(\"Model Saved\")"
      ],
      "metadata": {
        "id": "U7sZWJm8Pk40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet_model = SimpleUNet().to(device)\n",
        "unet_model.load_state_dict(torch.load('genImage25epoch.pth'))\n",
        "unet_model.eval()"
      ],
      "metadata": {
        "id": "epdabvCCPnXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxSteps = 1000\n",
        "noiseScheduler = DDPMScheduler(num_train_timesteps=maxSteps,\n",
        "                               beta_start=0.001,\n",
        "                               beta_end=0.02)\n",
        "def genImage(model,num_images = 5):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    noise = torch.rand(num_images, 3, 64, 64).to(device)\n",
        "    for i in reversed(range(max_timesteps)):\n",
        "      timestep = torch.full((num_images,), i, device = device)\n",
        "      predicted_noise = model(noise, timestep)\n",
        "      noise = noiseScheduler.step(predicted_noise, i, noise).prev_sample\n",
        "    return noise.cpu().numpy()\n",
        "def plot_images(timesteps, images, title=\"Images\"):\n",
        "    fig, axes = plt.subplots(1, len(timesteps), figsize=(15, 3))\n",
        "    for i, (t, img) in enumerate(zip(timesteps, images)):\n",
        "        img_display = np.transpose(img, (1, 2, 0))\n",
        "        img_display = np.clip(img_display, 0, 1)\n",
        "        axes[i].imshow(img_display)\n",
        "        axes[i].set_title(f\"t={t}\")\n",
        "        axes[i].axis('off')\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "generated = genImage(unet_model, 5)\n",
        "print(generated.shape)\n",
        "plot_images = (range(5), generated, \"Generated Images\")"
      ],
      "metadata": {
        "id": "HlSob9x_PoBQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}